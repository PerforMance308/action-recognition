{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"3\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "import json\n",
    "import functools\n",
    "from PIL import Image\n",
    "import argparse\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "try:\n",
    "    import accimage\n",
    "except ImportError:\n",
    "    accimage = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_loader(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        with Image.open(f) as img:\n",
    "            return img.convert('RGB')\n",
    "        \n",
    "def get_default_image_loader():\n",
    "    from torchvision import get_image_backend\n",
    "    if get_image_backend() == 'accimage':\n",
    "        return accimage_loader\n",
    "    else:\n",
    "        return pil_loader\n",
    "\n",
    "def accimage_loader(path):\n",
    "    try:\n",
    "        return accimage.Image(path)\n",
    "    except IOError:\n",
    "        # Potentially a decoding problem, fall back to PIL.Image\n",
    "        return pil_loader(path)\n",
    "\n",
    "    \n",
    "def get_default_video_loader():\n",
    "    image_loader = get_default_image_loader()\n",
    "    return functools.partial(video_loader, image_loader=image_loader)\n",
    "\n",
    "def load_annotation_data(path):\n",
    "    with open(path, 'r') as data_file:\n",
    "        return json.load(data_file)\n",
    "    \n",
    "def load_value_file(file_path):\n",
    "    with open(file_path, 'r') as input_file:\n",
    "        value = float(input_file.read().rstrip('\\n\\r'))\n",
    "\n",
    "    return value\n",
    "\n",
    "def video_loader(video_dir_path, frame_indices, image_loader):\n",
    "    video = []\n",
    "    for i in frame_indices:\n",
    "        image_path = os.path.join(video_dir_path, 'image_{:05d}.jpg'.format(i))\n",
    "        if os.path.exists(image_path):\n",
    "            video.append(image_loader(image_path))\n",
    "        else:\n",
    "            return video\n",
    "\n",
    "    return video\n",
    "\n",
    "def get_video_names_and_annotations(data, subset):\n",
    "    video_names = []\n",
    "    annotations = []\n",
    "\n",
    "    for key, value in data['database'].items():\n",
    "        this_subset = value['subset']\n",
    "        if this_subset == subset:\n",
    "            label = value['annotations']['label']\n",
    "            video_names.append('{}/{}'.format(label, key))\n",
    "            annotations.append(value['annotations'])\n",
    "\n",
    "    return video_names, annotations\n",
    "\n",
    "def get_class_labels(data):\n",
    "    class_labels_map = {}\n",
    "    index = 0\n",
    "    for class_label in data['labels']:\n",
    "        class_labels_map[class_label] = index\n",
    "        index += 1\n",
    "    return class_labels_map\n",
    "    \n",
    "def make_dataset(root_path, annotation_path, subset, n_samples_for_each_video, sample_duration):\n",
    "    data = load_annotation_data(annotation_path)\n",
    "    video_names, annotations = get_video_names_and_annotations(data, subset)\n",
    "    class_to_idx = get_class_labels(data)\n",
    "    idx_to_class = {}\n",
    "    for name, label in class_to_idx.items():\n",
    "        idx_to_class[label] = name\n",
    "        \n",
    "    dataset = []\n",
    "    for i in range(len(video_names)):\n",
    "        if i % 1000 == 0:\n",
    "            print('dataset loading [{}/{}]'.format(i, len(video_names)))\n",
    "\n",
    "        video_path = os.path.join(root_path, video_names[i])\n",
    "        \n",
    "        if not os.path.exists(video_path):\n",
    "            continue\n",
    "\n",
    "        n_frames_file_path = os.path.join(video_path, 'n_frames')\n",
    "        n_frames = int(load_value_file(n_frames_file_path))\n",
    "        if n_frames <= 0:\n",
    "            continue\n",
    "\n",
    "        begin_t = 1\n",
    "        end_t = n_frames\n",
    "        sample = {\n",
    "            'video': video_path,\n",
    "            'segment': [begin_t, end_t],\n",
    "            'n_frames': n_frames,\n",
    "            'video_id': video_names[i].split('/')[1]\n",
    "        }\n",
    "        if len(annotations) != 0:\n",
    "            sample['label'] = class_to_idx[annotations[i]['label']]\n",
    "        else:\n",
    "            sample['label'] = -1\n",
    "\n",
    "        if n_samples_for_each_video == 1:\n",
    "            sample['frame_indices'] = list(range(1, n_frames + 1))\n",
    "            dataset.append(sample)\n",
    "        else:\n",
    "            if n_samples_for_each_video > 1:\n",
    "                step = max(1,\n",
    "                           math.ceil((n_frames - 1 - sample_duration) /\n",
    "                                     (n_samples_for_each_video - 1)))\n",
    "            else:\n",
    "                step = sample_duration\n",
    "            for j in range(1, n_frames, step):\n",
    "                sample_j = copy.deepcopy(sample)\n",
    "                sample_j['frame_indices'] = list(\n",
    "                    range(j, min(n_frames + 1, j + sample_duration)))\n",
    "                dataset.append(sample_j)\n",
    "\n",
    "    return dataset, idx_to_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCF101(data.Dataset):\n",
    "    def __init__(self, root_path, annotation_path, subset, spatial_transform, temporal_transform, target_transform, sample_duration = 16, get_loader=get_default_video_loader,\n",
    "                 n_samples_for_each_video=1):\n",
    "        self.data, self.class_names = make_dataset(root_path, annotation_path, subset, n_samples_for_each_video, sample_duration)\n",
    "        self.spatial_transform = spatial_transform \n",
    "        self.temporal_transform = temporal_transform \n",
    "        self.target_transform = target_transform\n",
    "        self.loader = get_loader()\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        path = self.data[index]['video']\n",
    "\n",
    "        frame_indices = self.data[index]['frame_indices']\n",
    "        if self.temporal_transform is not None:\n",
    "            frame_indices = self.temporal_transform(frame_indices)\n",
    "        clip = self.loader(path, frame_indices)\n",
    "        if self.spatial_transform is not None:\n",
    "            self.spatial_transform.randomize_parameters()\n",
    "            clip = [self.spatial_transform(img) for img in clip]\n",
    "        clip = torch.stack(clip, 0).permute(1, 0, 2, 3)\n",
    "\n",
    "        target = self.data[index]\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return clip, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img):\n",
    "        for t in self.transforms:\n",
    "            img = t(img)\n",
    "        return img\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        for t in self.transforms:\n",
    "            t.randomize_parameters()\n",
    "            \n",
    "class ClassLabel(object):\n",
    "    def __call__(self, target):\n",
    "        return target['label']\n",
    "    \n",
    "class TemporalRandomCrop(object):\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, frame_indices):\n",
    "        rand_end = max(0, len(frame_indices) - self.size - 1)\n",
    "        begin_index = random.randint(0, rand_end)\n",
    "        end_index = min(begin_index + self.size, len(frame_indices))\n",
    "\n",
    "        out = frame_indices[begin_index:end_index]\n",
    "\n",
    "        for index in out:\n",
    "            if len(out) >= self.size:\n",
    "                break\n",
    "            out.append(index)\n",
    "\n",
    "        return out\n",
    "\n",
    "class RandomHorizontalFlip(object):\n",
    "    def __call__(self, img):\n",
    "        if self.p < 0.5:\n",
    "            return img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        return img\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        self.p = random.random()\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __init__(self, norm_value=255):\n",
    "        self.norm_value = norm_value\n",
    "\n",
    "    def __call__(self, pic):\n",
    "        if isinstance(pic, np.ndarray):\n",
    "            # handle numpy array\n",
    "            img = torch.from_numpy(pic.transpose((2, 0, 1)))\n",
    "            # backward compatibility\n",
    "            return img.float().div(self.norm_value)\n",
    "\n",
    "        if accimage is not None and isinstance(pic, accimage.Image):\n",
    "            nppic = np.zeros(\n",
    "                [pic.channels, pic.height, pic.width], dtype=np.float32)\n",
    "            pic.copyto(nppic)\n",
    "            return torch.from_numpy(nppic)\n",
    "\n",
    "        # handle PIL Image\n",
    "        if pic.mode == 'I':\n",
    "            img = torch.from_numpy(np.array(pic, np.int32, copy=False))\n",
    "        elif pic.mode == 'I;16':\n",
    "            img = torch.from_numpy(np.array(pic, np.int16, copy=False))\n",
    "        else:\n",
    "            img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n",
    "        # PIL image mode: 1, L, P, I, F, RGB, YCbCr, RGBA, CMYK\n",
    "        if pic.mode == 'YCbCr':\n",
    "            nchannel = 3\n",
    "        elif pic.mode == 'I;16':\n",
    "            nchannel = 1\n",
    "        else:\n",
    "            nchannel = len(pic.mode)\n",
    "        img = img.view(pic.size[1], pic.size[0], nchannel)\n",
    "        # put it from HWC to CHW format\n",
    "        # yikes, this transpose takes 80% of the loading time/CPU\n",
    "        img = img.transpose(0, 1).transpose(0, 2).contiguous()\n",
    "        if isinstance(img, torch.ByteTensor):\n",
    "            return img.float().div(self.norm_value)\n",
    "        else:\n",
    "            return img\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        pass\n",
    "\n",
    "class Normalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        # TODO: make efficient\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.sub_(m).div_(s)\n",
    "        return tensor\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        pass\n",
    "\n",
    "class MultiScaleCornerCrop(object):\n",
    "    def __init__(self,\n",
    "                 scales,\n",
    "                 size,\n",
    "                 interpolation=Image.BILINEAR,\n",
    "                 crop_positions=['c', 'tl', 'tr', 'bl', 'br']):\n",
    "        self.scales = scales\n",
    "        self.size = size\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "        self.crop_positions = crop_positions\n",
    "\n",
    "    def __call__(self, img):\n",
    "        min_length = min(img.size[0], img.size[1])\n",
    "        crop_size = int(min_length * self.scale)\n",
    "\n",
    "        image_width = img.size[0]\n",
    "        image_height = img.size[1]\n",
    "\n",
    "        if self.crop_position == 'c':\n",
    "            center_x = image_width // 2\n",
    "            center_y = image_height // 2\n",
    "            box_half = crop_size // 2\n",
    "            x1 = center_x - box_half\n",
    "            y1 = center_y - box_half\n",
    "            x2 = center_x + box_half\n",
    "            y2 = center_y + box_half\n",
    "        elif self.crop_position == 'tl':\n",
    "            x1 = 0\n",
    "            y1 = 0\n",
    "            x2 = crop_size\n",
    "            y2 = crop_size\n",
    "        elif self.crop_position == 'tr':\n",
    "            x1 = image_width - crop_size\n",
    "            y1 = 0\n",
    "            x2 = image_width\n",
    "            y2 = crop_size\n",
    "        elif self.crop_position == 'bl':\n",
    "            x1 = 0\n",
    "            y1 = image_height - crop_size\n",
    "            x2 = crop_size\n",
    "            y2 = image_height\n",
    "        elif self.crop_position == 'br':\n",
    "            x1 = image_width - crop_size\n",
    "            y1 = image_height - crop_size\n",
    "            x2 = image_width\n",
    "            y2 = image_height\n",
    "\n",
    "        img = img.crop((x1, y1, x2, y2))\n",
    "\n",
    "        return img.resize((self.size, self.size), self.interpolation)\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        self.scale = self.scales[random.randint(0, len(self.scales) - 1)]\n",
    "        self.crop_position = self.crop_positions[random.randint(\n",
    "            0,\n",
    "            len(self.crop_positions) - 1)]\n",
    "\n",
    "class Scale(object):\n",
    "    def __init__(self, size, interpolation=Image.BILINEAR):\n",
    "        assert isinstance(size,\n",
    "                          int) or (isinstance(size, collections.Iterable) and\n",
    "                                   len(size) == 2)\n",
    "        self.size = size\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if isinstance(self.size, int):\n",
    "            w, h = img.size\n",
    "            if (w <= h and w == self.size) or (h <= w and h == self.size):\n",
    "                return img\n",
    "            if w < h:\n",
    "                ow = self.size\n",
    "                oh = int(self.size * h / w)\n",
    "                return img.resize((ow, oh), self.interpolation)\n",
    "            else:\n",
    "                oh = self.size\n",
    "                ow = int(self.size * w / h)\n",
    "                return img.resize((ow, oh), self.interpolation)\n",
    "        else:\n",
    "            return img.resize(self.size, self.interpolation)\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        pass\n",
    "\n",
    "class CornerCrop(object):\n",
    "\n",
    "    def __init__(self, size, crop_position=None):\n",
    "        self.size = size\n",
    "        if crop_position is None:\n",
    "            self.randomize = True\n",
    "        else:\n",
    "            self.randomize = False\n",
    "        self.crop_position = crop_position\n",
    "        self.crop_positions = ['c', 'tl', 'tr', 'bl', 'br']\n",
    "\n",
    "    def __call__(self, img):\n",
    "        image_width = img.size[0]\n",
    "        image_height = img.size[1]\n",
    "\n",
    "        if self.crop_position == 'c':\n",
    "            th, tw = (self.size, self.size)\n",
    "            x1 = int(round((image_width - tw) / 2.))\n",
    "            y1 = int(round((image_height - th) / 2.))\n",
    "            x2 = x1 + tw\n",
    "            y2 = y1 + th\n",
    "        elif self.crop_position == 'tl':\n",
    "            x1 = 0\n",
    "            y1 = 0\n",
    "            x2 = self.size\n",
    "            y2 = self.size\n",
    "        elif self.crop_position == 'tr':\n",
    "            x1 = image_width - self.size\n",
    "            y1 = 0\n",
    "            x2 = image_width\n",
    "            y2 = self.size\n",
    "        elif self.crop_position == 'bl':\n",
    "            x1 = 0\n",
    "            y1 = image_height - self.size\n",
    "            x2 = self.size\n",
    "            y2 = image_height\n",
    "        elif self.crop_position == 'br':\n",
    "            x1 = image_width - self.size\n",
    "            y1 = image_height - self.size\n",
    "            x2 = image_width\n",
    "            y2 = image_height\n",
    "\n",
    "        img = img.crop((x1, y1, x2, y2))\n",
    "\n",
    "        return img\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        if self.randomize:\n",
    "            self.crop_position = self.crop_positions[random.randint(\n",
    "                0,\n",
    "                len(self.crop_positions) - 1)]\n",
    "\n",
    "class LoopPadding(object):\n",
    "\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, frame_indices):\n",
    "        out = frame_indices\n",
    "\n",
    "        for index in out:\n",
    "            if len(out) >= self.size:\n",
    "                break\n",
    "            out.append(index)\n",
    "\n",
    "        return out\n",
    "    \n",
    "class VideoID(object):\n",
    "    def __call__(self, target):\n",
    "        return target['video_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_set(video_path, annotation_path, spatial_transform, temporal_transform, target_transform):\n",
    "    training_data = UCF101(\n",
    "            video_path,\n",
    "            annotation_path,\n",
    "            'training',\n",
    "            spatial_transform, \n",
    "            temporal_transform, \n",
    "            target_transform)\n",
    "    return training_data\n",
    "\n",
    "def get_test_set(video_path, annotation_path, spatial_transform, temporal_transform, target_transform):    \n",
    "    test_data = UCF101(\n",
    "            video_path,\n",
    "            annotation_path,\n",
    "            'validation',\n",
    "            spatial_transform,\n",
    "            temporal_transform,\n",
    "            target_transform)\n",
    "\n",
    "    return test_data\n",
    "\n",
    "def get_mean(norm_value=255):\n",
    "    return [114.7748 / norm_value, 107.7354 / norm_value, 99.4750 / norm_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset loading [0/9537]\n",
      "dataset loading [1000/9537]\n",
      "dataset loading [2000/9537]\n",
      "dataset loading [3000/9537]\n",
      "dataset loading [4000/9537]\n",
      "dataset loading [5000/9537]\n",
      "dataset loading [6000/9537]\n",
      "dataset loading [7000/9537]\n",
      "dataset loading [8000/9537]\n",
      "dataset loading [9000/9537]\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "scales = [1]\n",
    "for i in range(1, 5):\n",
    "    scales.append(scales[-1] * 0.84089641525)\n",
    "    \n",
    "mean = get_mean(1)\n",
    "norm_method = Normalize(mean, [1, 1, 1])\n",
    "\n",
    "spatial_transform = Compose([\n",
    "            MultiScaleCornerCrop(scales, 56),\n",
    "            RandomHorizontalFlip(),\n",
    "            ToTensor(1), norm_method\n",
    "        ])\n",
    "temporal_transform = TemporalRandomCrop(16)\n",
    "target_transform = ClassLabel()\n",
    "\n",
    "training_data = get_training_set('./data/ucf101/jpg', './data/ucf101_01.json', spatial_transform, temporal_transform, target_transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "            training_data,\n",
    "            batch_size=64,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import resnet\n",
    "import resnext\n",
    "from torch import nn\n",
    "def generate_model():\n",
    "    model = resnext.resnet101(\n",
    "                num_classes=101,\n",
    "                shortcut_type='A',\n",
    "                sample_size=56,\n",
    "                sample_duration=16)\n",
    "\n",
    "    model = model.cuda()\n",
    "    model = nn.DataParallel(model, device_ids=None)\n",
    "\n",
    "    return model, model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(outputs, targets):\n",
    "    batch_size = targets.size(0)\n",
    "\n",
    "    _, pred = outputs.topk(1, 1, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(targets.view(1, -1))\n",
    "    n_correct_elems = correct.float().sum().data[0]\n",
    "\n",
    "    return n_correct_elems / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "def train_epoch(epoch, data_loader, model, criterion, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    for i, (inputs, targets) in enumerate(data_loader):\n",
    "        targets = targets.cuda(async=True)\n",
    "        inputs = Variable(inputs)\n",
    "        targets = Variable(targets)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        acc = calculate_accuracy(outputs, targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print('loss: ', loss.data, ', acc: ', acc.data)\n",
    "        \n",
    "        save_file_path = os.path.join('./data/save',\n",
    "                                      'save_{}.pth'.format(epoch))\n",
    "        states = {\n",
    "            'epoch': epoch + 1,\n",
    "            'arch': opt.arch,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }\n",
    "        torch.save(states, save_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shiqi/Action/resnext.py:121: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  m.weight = nn.init.kaiming_normal(m.weight, mode='fan_out')\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "model, parameters = generate_model()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.cuda()\n",
    "\n",
    "optimizer = optim.SGD(\n",
    "            parameters,\n",
    "            lr=0.1,\n",
    "            momentum=0.9,\n",
    "            dampening=0.9,\n",
    "            weight_decay=0.001,\n",
    "            nesterov=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:7: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(5.9983, device='cuda:0') , acc:  tensor(0.0156, device='cuda:0')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'opt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b2e15805f8cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"start epoch:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-b140f25fece2>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(epoch, data_loader, model, criterion, optimizer)\u001b[0m\n\u001b[1;32m     21\u001b[0m         states = {\n\u001b[1;32m     22\u001b[0m             \u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0;34m'arch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0march\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;34m'optimizer'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'opt' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(0, 300):\n",
    "    print(\"start epoch:\", i)\n",
    "    train_epoch(i, train_loader, model, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_video_results(output_buffer, video_id, test_results, class_names):\n",
    "    video_outputs = torch.stack(output_buffer)\n",
    "    average_scores = torch.mean(video_outputs, dim=0)\n",
    "    sorted_scores, locs = torch.topk(average_scores, k=1)\n",
    "\n",
    "    video_results = []\n",
    "    for i in range(sorted_scores.size(0)):\n",
    "        video_results.append({\n",
    "            'label': class_names[locs[i].item()],\n",
    "            'score': sorted_scores[i]\n",
    "        })\n",
    "\n",
    "    test_results['results'][video_id] = video_results\n",
    "    \n",
    "def calcacc(results):\n",
    "    acc = 0\n",
    "    for i in results:\n",
    "        label = test_data.class_names[i.item()]\n",
    "        pre = results[i][0]['label']\n",
    "        if label == pre:\n",
    "            acc += 1\n",
    "    return acc / len(results)\n",
    "    \n",
    "def test(data_loader, model, class_names):\n",
    "    model.eval()\n",
    "\n",
    "    output_buffer = []\n",
    "    previous_video_id = ''\n",
    "    test_results = {'results': {}}\n",
    "    for i, (inputs, targets) in enumerate(data_loader):\n",
    "        inputs = Variable(inputs, volatile=True)\n",
    "        outputs = model(inputs)\n",
    "        outputs = F.softmax(outputs)\n",
    "        for j in range(outputs.size(0)):\n",
    "            if not (i == 0 and j == 0) and targets[j] != previous_video_id:\n",
    "                calculate_video_results(output_buffer, previous_video_id,\n",
    "                                        test_results, class_names)\n",
    "                output_buffer = []\n",
    "            output_buffer.append(outputs[j].data.cpu())\n",
    "            previous_video_id = targets[j]\n",
    "    acc = calcacc(test_results['results'])\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_transform = Compose([\n",
    "            MultiScaleCornerCrop(scales, 56),\n",
    "            RandomHorizontalFlip(),\n",
    "            ToTensor(1), norm_method\n",
    "        ])\n",
    "temporal_transform = TemporalRandomCrop(16)\n",
    "target_transform = ClassLabel()\n",
    "\n",
    "test_data = get_test_set('./data/ucf101/jpg', './data/ucf101_01.json', spatial_transform, temporal_transform,\n",
    "                                 target_transform)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "            test_data,\n",
    "            batch_size=64,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            pin_memory=True)\n",
    "test_acc = test(test_loader, model, test_data.class_names)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
